{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "df=pd.read_csv(\"C:/Users/GOHIL RAJENDRASINH/Downloads/stack-overflow-developer-survey-2024/survey_results_public.csv\")\n",
    "df.head()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **EDA and Data Cleaning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Dropping rows which has null values in the target column(Compensation in USD) in the beginning as it has no use since, the purpose is to predict salary and the only required data are the rows which have the salary records.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned=df[df['ConvertedCompYearly'].notnull()]  #Dropping rows in which salary column has null values\n",
    "\n",
    "df_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Basic Information of Dataset after dropping null values of target i.e. ConvertedCompYearly:\\n\")\n",
    "\n",
    "df_cleaned.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = df_cleaned.dropna(subset=['EdLevel'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Selecting columns that are necessary for predicting income**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "necessary_df= df_cleaned[['ConvertedCompYearly','Age','EdLevel','YearsCodePro','Country','Industry','LanguageHaveWorkedWith','PlatformHaveWorkedWith','ToolsTechHaveWorkedWith','WorkExp']]\n",
    "\n",
    "necessary_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "necessary_df[\"Industry\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "necessary_df[\"Country\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "necessary_df[\"Country\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**There are a lot of countries in the dataset that doesn't have significant amount of developers hence, changing the country names to 'Other' for which the No. of developers are 500 or less to make one category**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shorten_categories(categories,cutoff):\n",
    "    categories_map = {}\n",
    "    for i in range(len(categories)):\n",
    "        if categories.values[i]>=cutoff:\n",
    "            categories_map[categories.index[i]] = categories.index[i]\n",
    "        else:\n",
    "            categories_map[categories.index[i]] = 'Other'\n",
    "    return categories_map\n",
    "\n",
    "country_map = shorten_categories(necessary_df['Country'].value_counts(),500)\n",
    "necessary_df[\"Country\"] = necessary_df[\"Country\"].map(country_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "necessary_df[\"Country\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 15))\n",
    "sns.countplot(y='Country', data=necessary_df, order=necessary_df['Country'].value_counts().index)\n",
    "plt.title('No. of Developers Country-wise')\n",
    "plt.xlabel('Number of Developers')\n",
    "plt.ylabel('Country')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "prog_Tools = necessary_df['ToolsTechHaveWorkedWith'].str.split(';', expand=True).stack().value_counts().nlargest(10)\n",
    "prog_Tools.plot(kind='barh', color='Blue')\n",
    "plt.title('Top 10 Most Popular Programming Tools')\n",
    "plt.xlabel('Number of Developers')\n",
    "plt.ylabel('Prog tools')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "popular_languages = necessary_df['LanguageHaveWorkedWith'].str.split(';', expand=True).stack().value_counts().nlargest(10)\n",
    "popular_languages.plot(kind='barh', color='Red')\n",
    "plt.title('Top 10 Most Popular Programming Languages')\n",
    "plt.xlabel('Number of Developers')\n",
    "plt.ylabel('Programming Language')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> # **Outlier Visualisation and Treatment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,1,figsize=(12,8))\n",
    "plt.ticklabel_format(style = 'plain')\n",
    "\n",
    "necessary_df.boxplot(\"ConvertedCompYearly\",\"Country\",ax=ax)\n",
    "plt.suptitle(\"Compensation V/s Country\")\n",
    "plt.title('Boxplot to visualise Outliers')\n",
    "plt.ylabel(\"Salary\")\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **On the basis of the boxplot above, it seems that there are lot of false values entered salary-wise, one of the best way to identify these are by spreading the compensation values across different countries to get a good understanding of the outliers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = necessary_df.groupby('Country')['ConvertedCompYearly'].describe().reset_index()[['Country','25%','75%']]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "necessary_df = necessary_df.merge(data, on = \"Country\",how =\"left\")\n",
    "necessary_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#slightly adapted from https://www.kaggle.com/code/pavithrasivan98/salaryprediction\n",
    "\n",
    "mask = necessary_df[\"ConvertedCompYearly\"]<necessary_df['25%']\n",
    "necessary_df.loc[mask,\"ConvertedCompYearly\"] = necessary_df[\"25%\"]\n",
    "\n",
    "mask = necessary_df[\"ConvertedCompYearly\"]>necessary_df['75%']\n",
    "necessary_df.loc[mask,\"ConvertedCompYearly\"] = necessary_df[\"75%\"]\n",
    "\n",
    "necessary_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "necessary_df.drop(['25%','75%'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "necessary_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,1,figsize=(15,10))\n",
    "plt.ticklabel_format(style = 'plain')\n",
    "\n",
    "necessary_df.boxplot(\"ConvertedCompYearly\",\"Country\",ax=ax)\n",
    "plt.suptitle(\"Compensation V/s Country\")\n",
    "plt.title('Boxplot to visualise Outliers')\n",
    "plt.ylabel(\"Salary\")\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "necessary_df['Compensation'] = 'Low Compensation'\n",
    "necessary_df.loc[necessary_df['ConvertedCompYearly'] > 50000, 'Compensation'] = 'High Compensation'\n",
    "necessary_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "sns.countplot(data=necessary_df,x='Compensation')\n",
    "plt.title('Salary-wise Distribution')\n",
    "plt.xlabel('Compensation')\n",
    "plt.ylabel('No. of developers')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **checking value counts of target column to see if the data is reliable for just judging through Accuracy, otherwise Confusion Matrix could be used**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(necessary_df['Compensation'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# necessary_df.drop(['ConvertedCompYearly'],axis=1,inplace=True)\n",
    "\n",
    "necessary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Basic Information of the new Dataset with only selected features:\\n\",necessary_df.info())\n",
    "print(\"\\n\\nDescription of the new Dataset with only selected features:\\n\",necessary_df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> # **Visualizing and Eliminating Duplicate Rows**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "necessary_df.duplicated().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicate = necessary_df[necessary_df.duplicated()]\n",
    "duplicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "necessary_df = necessary_df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "necessary_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> # **Analyzing and Eliminating Null values with appropriate methods for each column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"null values:\\n\",necessary_df.isnull().sum().sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(necessary_df['Industry'].unique())\n",
    "print(necessary_df['Industry'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **The Industry column has an 'other' category hence, replacing 'NaN' values with 'other' as both are undefined inputs. It will result in eliminating null values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "necessary_df['Industry']=necessary_df['Industry'].fillna('Other')\n",
    "\n",
    "print(necessary_df['Industry'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "sns.countplot(data=necessary_df,y='Industry')\n",
    "plt.title('Industry-wise Distribution')\n",
    "plt.xlabel('No. of Developers')\n",
    "plt.ylabel('Industry')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Filling up null values of Work experince(numerical) column with median as it is less sensitive to the outliers as compared to mean.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "necessary_df['WorkExp']=necessary_df['WorkExp'].fillna(necessary_df['WorkExp'].median())\n",
    "\n",
    "print(\"null values:\\n\",necessary_df.isnull().sum().sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "necessary_df.isnull().sum().sort_values(ascending=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Filling up null values of 'dtype=object' columns with mode.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  you’re replacing missing values in those specific columns with that column’s most frequent value.\n",
    "column_with_null = [\"PlatformHaveWorkedWith\",\"ToolsTechHaveWorkedWith\",\"LanguageHaveWorkedWith\",\"YearsCodePro\"]\n",
    "necessary_df[column_with_null]=necessary_df[column_with_null].fillna(necessary_df.mode().iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "necessary_df.isnull().sum().sort_values(ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "necessary_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "necessary_df['Age'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Replacing age groups to their initial age number for better visualisation instead of applying encoder which might give them random labels.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_age=['25','45','35','17','55','18','65','0']\n",
    "\n",
    "necessary_df['Age'] = necessary_df['Age'].replace(['25-34 years old', '45-54 years old', '35-44 years old',\n",
    "       'Under 18 years old', '55-64 years old', '18-24 years old',\n",
    "       '65 years or older', 'Prefer not to say'], replace_age)\n",
    "\n",
    "necessary_df['Age'] = necessary_df['Age'].astype(int) #making sure all the values are integer by converting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#graph of age vs no. of developers\n",
    "plt.figure(figsize=(8,8))\n",
    "sns.countplot(data=necessary_df,x='Age')\n",
    "plt.title('Age Distribution')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('No. of Developers')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "necessary_df['EdLevel'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "sns.histplot(data=necessary_df, y='EdLevel')\n",
    "plt.title('Education Level Distribution')\n",
    "plt.xlabel('No. of Developers')\n",
    "plt.ylabel('Education Level')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Replacing Education level based on ranks in a descending order such as Ph.d=8,Master's=7 and so on for simplifying identification**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert into the number\n",
    "replace_edu=['6','4','7','2','8','5','3','1']\n",
    "necessary_df['EdLevel'] = necessary_df['EdLevel'].replace(['Bachelor’s degree (B.A., B.S., B.Eng., etc.)',\n",
    "       'Some college/university study without earning a degree',\n",
    "       'Master’s degree (M.A., M.S., M.Eng., MBA, etc.)',\n",
    "       'Primary/elementary school',\n",
    "       'Professional degree (JD, MD, Ph.D, Ed.D, etc.)',\n",
    "       'Associate degree (A.A., A.S., etc.)',\n",
    "       'Secondary school (e.g. American high school, German Realschule or Gymnasium, etc.)',\n",
    "       'Something else'], replace_edu)\n",
    "\n",
    "necessary_df['EdLevel'] = necessary_df['EdLevel'].astype(int)\n",
    "\n",
    "\n",
    "necessary_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "necessary_df['YearsCodePro'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Replacing the only two object values of Professional coding experience to convert dtype to int for further processing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "necessary_df['YearsCodePro'] = necessary_df['YearsCodePro'].replace('Less than 1 year', '1')\n",
    "necessary_df['YearsCodePro'] = necessary_df['YearsCodePro'].replace('More than 50 years', '51')\n",
    "\n",
    "necessary_df['YearsCodePro'] = necessary_df['YearsCodePro'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "necessary_df['WorkExp'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Converting float dtype to int by rounding off the values, *rounding off* so that the decimal values doesn't get eliminated as it is**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "necessary_df['WorkExp'] = necessary_df['WorkExp'].round().astype(int) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "necessary_df['WorkExp'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "sns.scatterplot(x='Age', y='YearsCodePro', hue='Compensation', data=necessary_df, palette='viridis', size='Compensation', sizes=(50, 200))\n",
    "plt.title('Relationship among Age, YearsCodePro, and Compensation')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('YearsCodePro')\n",
    "plt.legend(title='Compensation')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_columns = necessary_df[['WorkExp','YearsCodePro','EdLevel','Age']]\n",
    "\n",
    "correlation_matrix = corr_columns.corr()#creating a correlation matrix of selected columns\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".3f\", linewidths=0.5)\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Encoding**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Using Label Encoder to encode categorical columns, Using *'Label Encoding'* as there are a lot of different values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "LEnc=LabelEncoder()\n",
    "object_cols=['Compensation','Country','Industry','LanguageHaveWorkedWith','PlatformHaveWorkedWith','ToolsTechHaveWorkedWith']\n",
    "necessary_df[object_cols]=necessary_df[object_cols].apply(LEnc.fit_transform)\n",
    "necessary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_columns = necessary_df[['YearsCodePro','EdLevel','Age','Compensation']]\n",
    "\n",
    "correlation_matrix = corr_columns.corr()#creating a correlation matrix of selected columns\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".3f\", linewidths=0.5)\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Regression - Machine Learning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_name = 'Compensation'\n",
    "column_0 = necessary_df.pop(column_name)\n",
    "necessary_df.insert(0, column_name, column_0)\n",
    "necessary_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = necessary_df.iloc[:,2:]\n",
    "y = necessary_df.iloc[:,1] \n",
    "print(\"Features X\\n\",X[0:5])\n",
    "print(\"Target y\\n\", y[0:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **k-Nearest Neighbour**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sklearn.model_selection as model_selection\n",
    "# from sklearn.neighbors import KNeighborsRegressor\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn import metrics\n",
    "# import numpy as np\n",
    "\n",
    "# # Train-test split\n",
    "# X_train, X_test, y_train, y_test = model_selection.train_test_split(\n",
    "#     X, y, test_size=0.3, random_state=1\n",
    "# )\n",
    "\n",
    "# # Normalization\n",
    "# scaler = StandardScaler()\n",
    "# scaler.fit(X_train)\n",
    "# X_train = scaler.transform(X_train)\n",
    "# X_test = scaler.transform(X_test)\n",
    "\n",
    "# # Lists to store results\n",
    "# k_list = []\n",
    "# rmse_list = []\n",
    "# r2_list = []\n",
    "\n",
    "# # Loop over k values (20–25)\n",
    "# for k in range(20, 26):\n",
    "#     clf_knn = KNeighborsRegressor(\n",
    "#         n_neighbors=k, weights=\"distance\", metric=\"euclidean\"\n",
    "#     )\n",
    "#     clf_knn.fit(X_train, y_train)\n",
    "#     y_pred = clf_knn.predict(X_test)\n",
    "\n",
    "#     rmse = np.sqrt(metrics.mean_squared_error(y_test, y_pred))\n",
    "#     r2 = metrics.r2_score(y_test, y_pred)\n",
    "\n",
    "#     print(f\"k={k} → RMSE: {rmse:.2f}, R²: {r2:.3f}\")\n",
    "\n",
    "#     k_list.append(k)\n",
    "#     rmse_list.append(rmse)\n",
    "#     r2_list.append(r2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Decision Tree**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn import model_selection, metrics\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(\n",
    "    X, y, test_size=0.3, random_state=4\n",
    ")\n",
    "\n",
    "# Use regressor instead of classifier\n",
    "DT_reg = DecisionTreeRegressor(max_depth=3,min_impurity_decrease=0.01)\n",
    "DT_reg.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = DT_reg.predict(X_test)\n",
    "\n",
    "# Evaluate with regression metrics\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import numpy as np\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    # Works on all sklearn versions\n",
    "    try:\n",
    "        return mean_squared_error(y_true, y_pred, squared=False)  # new API\n",
    "    except TypeError:\n",
    "        return np.sqrt(mean_squared_error(y_true, y_pred))        # old API\n",
    "\n",
    "# After you predict:\n",
    "# y_pred = model.predict(X_test)\n",
    "print(\"RMSE:\", rmse(y_test, y_pred))\n",
    "print(\"MAE :\", mean_absolute_error(y_test, y_pred))\n",
    "print(\"R²  :\", r2_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grid searchCv :\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "param_grid = {\n",
    "    'max_depth': [5, 8, 12, 15],\n",
    "    'min_samples_split': [2, 5, 10, 15],\n",
    "    'min_samples_leaf': [1, 2, 4, 8],\n",
    "    'max_features': [None, 'sqrt', 'log2']\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(DecisionTreeRegressor(random_state=42), param_grid, cv=5, scoring='r2')\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best Parameters:\", grid.best_params_)\n",
    "best_model = grid.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn import metrics\n",
    "\n",
    "# Limit tree growth\n",
    "\n",
    "DT_reg = DecisionTreeRegressor(\n",
    "    max_depth=8,           # restrict tree depth\n",
    "    min_samples_split=2,  # need at least 10 samples to split\n",
    "    min_samples_leaf=8,    # each leaf must have 5 samples\n",
    "    random_state=42,\n",
    "    max_features=None\n",
    ")\n",
    "\n",
    "\n",
    "DT_reg.fit(X_train, y_train)\n",
    "\n",
    "y_pred = DT_reg.predict(X_test)\n",
    "# print(\"RMSE:\", metrics.mean_squared_error(y_test, y_pred, squared=False))\n",
    "print(\"R²  :\", metrics.r2_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#randomforestRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rf_model = RandomForestRegressor(\n",
    "    n_estimators=550, max_depth=17, min_samples_leaf=4, random_state=42,max_features='sqrt'\n",
    ")\n",
    "rf_model.fit(X_train, y_train)\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "print(\"R²  :\", r2_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "# from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
    "# from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# # Ensure X and y are standard pandas/numpy objects\n",
    "# X = pd.DataFrame(X) if not isinstance(X, pd.DataFrame) else X\n",
    "# y = pd.Series(y) if not isinstance(y, pd.Series) else y\n",
    "\n",
    "# # Train-test split\n",
    "# X_train, X_test, y_train, y_test = train_test_split(\n",
    "#     X, y, test_size=0.3, random_state=42\n",
    "# )\n",
    "\n",
    "# # Initialize Random Forest (class itself, not fitted instance)\n",
    "# rf = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# # Hyperparameter grid (smaller & faster than full GridSearch)\n",
    "# param_grid = {\n",
    "#     'n_estimators': [100, 200, 300, 400],\n",
    "#     'max_depth': [None, 10, 15, 20],\n",
    "#     'min_samples_split': [2, 5, 10],\n",
    "#     'min_samples_leaf': [1, 2, 4],\n",
    "#     'max_features': ['sqrt', 'log2']\n",
    "# }\n",
    "\n",
    "# # RandomizedSearchCV (picklable-safe, faster)\n",
    "# random_search = RandomizedSearchCV(\n",
    "#     estimator=rf,\n",
    "#     param_distributions=param_grid,\n",
    "#     n_iter=50,        # number of random combinations\n",
    "#     cv=3,             # 3-fold CV for speed\n",
    "#     scoring='r2',\n",
    "#     n_jobs=1,         # safe option to avoid BrokenProcessPool\n",
    "#     verbose=2,\n",
    "#     random_state=42\n",
    "# )\n",
    "\n",
    "# # Fit RandomizedSearchCV\n",
    "# random_search.fit(X_train, y_train)\n",
    "\n",
    "# # Best parameters\n",
    "# print(\"Best Parameters:\", random_search.best_params_)\n",
    "\n",
    "# # Best model\n",
    "# best_rf = random_search.best_estimator_\n",
    "\n",
    "# # Predictions on test set\n",
    "# y_pred = best_rf.predict(X_test)\n",
    "\n",
    "# # Evaluation\n",
    "# rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "# mae = mean_absolute_error(y_test, y_pred)\n",
    "# r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# print(f\"Test RMSE: {rmse:.2f}\")\n",
    "# print(f\"Test MAE : {mae:.2f}\")\n",
    "# print(f\"Test R²  : {r2:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "# from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
    "# from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "# from sklearn.compose import ColumnTransformer\n",
    "# from sklearn.pipeline import Pipeline\n",
    "\n",
    "# # ------------------------------\n",
    "# # Step 1: Outlier removal (target variable)\n",
    "# # ------------------------------\n",
    "# def remove_outliers(df, target_col):\n",
    "#     Q1 = df[target_col].quantile(0.25)\n",
    "#     Q3 = df[target_col].quantile(0.75)\n",
    "#     IQR = Q3 - Q1\n",
    "#     lower = Q1 - 1.5 * IQR\n",
    "#     upper = Q3 + 1.5 * IQR\n",
    "#     return df[(df[target_col] >= lower) & (df[target_col] <= upper)]\n",
    "\n",
    "# # Combine X and y temporarily\n",
    "# df = pd.concat([X, y.rename(\"target\")], axis=1)\n",
    "# df_clean = remove_outliers(df, \"target\")\n",
    "# y_clean = df_clean[\"target\"]\n",
    "# X_clean = df_clean.drop(columns=[\"target\"])\n",
    "\n",
    "# # ------------------------------\n",
    "# # Step 2: Encode categorical variables\n",
    "# # ------------------------------\n",
    "# cat_cols = X_clean.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "# num_cols = X_clean.select_dtypes(exclude=['object', 'category']).columns.tolist()\n",
    "\n",
    "# preprocessor = ColumnTransformer(\n",
    "#     transformers=[\n",
    "#         ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), cat_cols)\n",
    "#     ],\n",
    "#     remainder='passthrough'  # leave numeric columns as-is\n",
    "# )\n",
    "\n",
    "# # ------------------------------\n",
    "# # Step 3: Train-test split\n",
    "# # ------------------------------\n",
    "# X_train, X_test, y_train, y_test = train_test_split(\n",
    "#     X_clean, y_clean, test_size=0.3, random_state=42\n",
    "# )\n",
    "\n",
    "# # ------------------------------\n",
    "# # Step 4: Random Forest + RandomizedSearchCV\n",
    "# # ------------------------------\n",
    "# rf = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# param_grid = {\n",
    "#     'n_estimators': [200, 300, 400, 500, 600],\n",
    "#     'max_depth': [None, 15, 20, 25],\n",
    "#     'min_samples_split': [2, 5, 10],\n",
    "#     'min_samples_leaf': [1, 2, 4],\n",
    "#     'max_features': ['sqrt', 'log2']\n",
    "# }\n",
    "\n",
    "# pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "#                            ('regressor', rf)])\n",
    "\n",
    "# random_search = RandomizedSearchCV(\n",
    "#     estimator=pipeline,\n",
    "#     param_distributions={\n",
    "#         'regressor__' + key: value for key, value in param_grid.items()\n",
    "#     },\n",
    "#     n_iter=50,\n",
    "#     cv=3,\n",
    "#     scoring='r2',\n",
    "#     n_jobs=1,  # safe option\n",
    "#     verbose=2,\n",
    "#     random_state=42\n",
    "# )\n",
    "\n",
    "# # Fit RandomizedSearchCV\n",
    "# random_search.fit(X_train, y_train)\n",
    "\n",
    "# # ------------------------------\n",
    "# # Step 5: Evaluation\n",
    "# # ------------------------------\n",
    "# best_model = random_search.best_estimator_\n",
    "# y_pred = best_model.predict(X_test)\n",
    "\n",
    "# rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "# mae = mean_absolute_error(y_test, y_pred)\n",
    "# r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# print(\"Best Parameters:\", random_search.best_params_)\n",
    "# print(f\"Test RMSE: {rmse:.2f}\")\n",
    "# print(f\"Test MAE : {mae:.2f}\")\n",
    "# print(f\"Test R²  : {r2:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Logistic Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = necessary_df.iloc[:,2:]\n",
    "y = necessary_df.iloc[:,1] \n",
    "print(\"Features X\\n\",X[0:5])\n",
    "print(\"Target y\\n\", y[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Linear Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Linear Regression \n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn import metrics\n",
    "from sklearn import model_selection\n",
    "import numpy as np\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(\n",
    "    X, y, test_size=0.3, random_state=4\n",
    ")\n",
    "\n",
    "# Scale features\n",
    "scaler = MinMaxScaler() \n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Regression model\n",
    "LR_reg = LinearRegression()\n",
    "LR_reg.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = LR_reg.predict(X_test)\n",
    "\n",
    "# RMSE function (works for all sklearn versions)\n",
    "def rmse(y_true, y_pred):\n",
    "    try:\n",
    "        return metrics.mean_squared_error(y_true, y_pred, squared=False)  # New API\n",
    "    except TypeError:\n",
    "        return np.sqrt(metrics.mean_squared_error(y_true, y_pred))        # Old API\n",
    "\n",
    "# Evaluate regression performance\n",
    "print(\"RMSE:\", rmse(y_test, y_pred))\n",
    "print(\"MAE :\", metrics.mean_absolute_error(y_test, y_pred))\n",
    "print(\"R²  :\", metrics.r2_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Evaluation of Regressior**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LR_reg = LinearRegression()\n",
    "# from  sklearn.metrics  import RocCurveDisplay\n",
    "# roc_lr  = RocCurveDisplay.from_estimator(LR_reg , X_test , y_test)\n",
    "# roc_dt  = RocCurveDisplay.from_estimator( DT_reg , X_test , y_test , ax= roc_lr. ax_)\n",
    "# # roc_knn  = RocCurveDisplay.from_estimator( clf_reg , X_test , y_test , ax= roc_lr. ax_)\n",
    "\n",
    "# #As per the ROC curve below Logistic regression has the best performance followed by KNN on the basis of Area Under the Curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Ensemble Learning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Voting Method**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "# Example regressor ensemble\n",
    "voting_reg = VotingRegressor(estimators=[\n",
    "    ('dt', DT_reg),        # DecisionTreeRegressor\n",
    "    # ('knn', knn_reg),      # KNeighborsRegressor\n",
    "    ('lr', LR_reg)         # LinearRegression\n",
    "])\n",
    "\n",
    "voting_reg.fit(X_train, y_train)\n",
    "y_pred = voting_reg.predict(X_test)\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(\"RMSE:\", rmse)\n",
    "print(\"R² Score:\", r2_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Random Forest Technique**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(\n",
    "    X, y, test_size=0.3, random_state=4\n",
    ")\n",
    "\n",
    "# Train regression model\n",
    "rf_reg = RandomForestRegressor(n_estimators=15, random_state=1)\n",
    "rf_reg.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_train_pred = rf_reg.predict(X_train)\n",
    "y_test_pred = rf_reg.predict(X_test)\n",
    "\n",
    "# Evaluate with regression metrics\n",
    "train_rmse = np.sqrt(metrics.mean_squared_error(y_train, y_train_pred))\n",
    "test_rmse = np.sqrt(metrics.mean_squared_error(y_test, y_test_pred))\n",
    "train_r2 = metrics.r2_score(y_train, y_train_pred)   # Added\n",
    "test_r2 = metrics.r2_score(y_test, y_test_pred)\n",
    "\n",
    "print(f\"Train RMSE: {train_rmse:.2f}, R²: {train_r2:.3f}\")\n",
    "print(f\"Test RMSE : {test_rmse:.2f}, R²: {test_r2:.3f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Bagging Method**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Decision Tree as base estimator for bagging below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(\n",
    "    X, y, test_size=0.3, random_state=4\n",
    ")\n",
    "\n",
    "# Train Bagging Regressor\n",
    "clf_ensemble_Bagging = BaggingRegressor(estimator=DT_reg, n_estimators=350, random_state=1)\n",
    "\n",
    "clf_ensemble_Bagging.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_train_pred = clf_ensemble_Bagging.predict(X_train)\n",
    "y_test_pred = clf_ensemble_Bagging.predict(X_test)\n",
    "\n",
    "# Evaluate for regression\n",
    "train_rmse = np.sqrt(metrics.mean_squared_error(y_train, y_train_pred))\n",
    "test_rmse = np.sqrt(metrics.mean_squared_error(y_test, y_test_pred))\n",
    "train_r2 = metrics.r2_score(y_train, y_train_pred)\n",
    "test_r2 = metrics.r2_score(y_test, y_test_pred)\n",
    "\n",
    "print(f\"Train RMSE: {train_rmse:.2f}, R²: {train_r2:.3f}\")\n",
    "print(f\"Test RMSE : {test_rmse:.2f}, R²: {test_r2:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using KNN as base estimator for bagging below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train RMSE: 28365.04, R²: 0.635\n",
      "Test RMSE : 30100.52, R²: 0.590\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import model_selection, metrics\n",
    "import numpy as np\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(\n",
    "    X, y, test_size=0.3, random_state=3\n",
    ")\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# KNN base regressor\n",
    "clf_knn = KNeighborsRegressor(n_neighbors=15)\n",
    "\n",
    "# Bagging Regressor\n",
    "clf_ensemble_Bagging = BaggingRegressor(\n",
    "    estimator=clf_knn,\n",
    "    n_estimators=50,\n",
    "    random_state=5,\n",
    "    max_samples=0.8,\n",
    "    max_features=0.8\n",
    ")\n",
    "\n",
    "# Train model\n",
    "clf_ensemble_Bagging.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_train_pred = clf_ensemble_Bagging.predict(X_train_scaled)\n",
    "y_test_pred = clf_ensemble_Bagging.predict(X_test_scaled)\n",
    "\n",
    "# Regression metrics\n",
    "train_rmse = np.sqrt(metrics.mean_squared_error(y_train, y_train_pred))\n",
    "test_rmse = np.sqrt(metrics.mean_squared_error(y_test, y_test_pred))\n",
    "train_r2 = metrics.r2_score(y_train, y_train_pred)\n",
    "test_r2 = metrics.r2_score(y_test, y_test_pred)\n",
    "\n",
    "print(f\"Train RMSE: {train_rmse:.2f}, R²: {train_r2:.3f}\")\n",
    "print(f\"Test RMSE : {test_rmse:.2f}, R²: {test_r2:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DecisionTreeRegressor with Bagging\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "\n",
    "dt = DecisionTreeRegressor(random_state=42, max_depth=10, min_samples_leaf=2)\n",
    "\n",
    "bagging = BaggingRegressor(\n",
    "    estimator=dt,\n",
    "    n_estimators=50,  # more trees\n",
    "    random_state=42,\n",
    "    max_samples=0.8,  # each tree trained on 80% of data\n",
    "    max_features=0.8\n",
    ")\n",
    "\n",
    "bagging.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_train_pred = bagging.predict(X_train)\n",
    "y_test_pred = bagging.predict(X_test)\n",
    "\n",
    "# R² scores\n",
    "train_r2 = metrics.r2_score(y_train, y_train_pred)\n",
    "test_r2 = metrics.r2_score(y_test, y_test_pred)\n",
    "\n",
    "# RMSE scores\n",
    "train_rmse = np.sqrt(metrics.mean_squared_error(y_train, y_train_pred))\n",
    "test_rmse = np.sqrt(metrics.mean_squared_error(y_test, y_test_pred))\n",
    "\n",
    "print(f\"Train RMSE: {train_rmse:.2f}, R²: {train_r2:.3f}\")\n",
    "print(f\"Test  RMSE: {test_rmse:.2f}, R²: {test_r2:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train RMSE: 19465.90\n",
      "Test  RMSE: 22216.83\n",
      "Train R² Score: 0.828\n",
      "Test  R² Score: 0.776\n"
     ]
    }
   ],
   "source": [
    "#as the GridSearchCV will time consuming\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import metrics\n",
    "\n",
    "# Model\n",
    "rf = RandomForestRegressor(\n",
    "    n_estimators=500,\n",
    "    max_depth=12,\n",
    "    min_samples_leaf=5,\n",
    "    random_state=42,\n",
    "    min_samples_split=8,\n",
    "    max_features='sqrt'\n",
    ")\n",
    "\n",
    "# Train\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_train_pred = rf.predict(X_train)\n",
    "y_test_pred = rf.predict(X_test)\n",
    "\n",
    "# R² scores\n",
    "train_r2 = metrics.r2_score(y_train, y_train_pred)\n",
    "test_r2 = metrics.r2_score(y_test, y_test_pred)\n",
    "\n",
    "train_rmse = np.sqrt(metrics.mean_squared_error(y_train, y_train_pred))\n",
    "test_rmse = np.sqrt(metrics.mean_squared_error(y_test, y_test_pred))\n",
    "\n",
    "print(f\"Train RMSE: {train_rmse:.2f}\")\n",
    "print(f\"Test  RMSE: {test_rmse:.2f}\")\n",
    "\n",
    "print(f\"Train R² Score: {train_r2:.3f}\")\n",
    "print(f\"Test  R² Score: {test_r2:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Running fast parameter search...\n",
      "✅ Best parameters (fast search): {'min_samples_split': 15, 'min_samples_leaf': 4, 'max_features': 0.8, 'max_depth': 15}\n",
      "🚀 Retraining with best parameters and 500 trees...\n",
      "\n",
      "📊 Final Model Performance:\n",
      "Train RMSE: 16861.78, R²: 0.871\n",
      "Test  RMSE: 20866.17, R²: 0.803\n",
      "[ 74656.5740886   65109.7296169  120610.5128602  ... 153802.83925331\n",
      "  28392.15312548  62220.48375166]\n"
     ]
    }
   ],
   "source": [
    "#After the apply the randomized search(as GridSearchCV are to time taking)\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "\n",
    "# Stage 1: Fast parameter search\n",
    "param_dist = {\n",
    "    'max_depth': [10, 12, 15],\n",
    "    'min_samples_leaf': [2, 4, 6],\n",
    "    'min_samples_split': [5, 10, 15],\n",
    "    'max_features': ['sqrt', 0.8, 1.0]\n",
    "}\n",
    "\n",
    "rand_search = RandomizedSearchCV(\n",
    "    RandomForestRegressor(n_estimators=100, random_state=42),  # Fewer trees for speed\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=15,           # Try only 15 random combos\n",
    "    cv=3,                # 3-fold CV for speed\n",
    "    scoring='r2',\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"🔍 Running fast parameter search...\")\n",
    "rand_search.fit(X_train, y_train)\n",
    "best_params = rand_search.best_params_\n",
    "print(\"✅ Best parameters (fast search):\", best_params)\n",
    "\n",
    "# Stage 2: Retrain with 500 trees using best parameters\n",
    "rf_final = RandomForestRegressor(\n",
    "    n_estimators=500,\n",
    "    random_state=42,\n",
    "    **best_params\n",
    ")\n",
    "\n",
    "print(\"🚀 Retraining with best parameters and 500 trees...\")\n",
    "rf_final.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_train_pred = rf_final.predict(X_train)\n",
    "y_test_pred = rf_final.predict(X_test)\n",
    "\n",
    "# Evaluation\n",
    "train_rmse = np.sqrt(metrics.mean_squared_error(y_train, y_train_pred))\n",
    "test_rmse = np.sqrt(metrics.mean_squared_error(y_test, y_test_pred))\n",
    "train_r2 = metrics.r2_score(y_train, y_train_pred)\n",
    "test_r2 = metrics.r2_score(y_test, y_test_pred)\n",
    "\n",
    "print(f\"\\n📊 Final Model Performance:\")\n",
    "print(f\"Train RMSE: {train_rmse:.2f}, R²: {train_r2:.3f}\")\n",
    "print(f\"Test  RMSE: {test_rmse:.2f}, R²: {test_r2:.3f}\")\n",
    "print(y_test_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Model saved to RandomForestRegressor(max_depth=15, max_features=0.8, min_samples_leaf=4,\n",
      "                      min_samples_split=15, n_estimators=500, random_state=42)\n",
      "✅ Model saved to rf_final_model.pkl and encoders saved to encoders.pkl\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# import pickle\n",
    "\n",
    "# le_country = LabelEncoder()\n",
    "# le_country.fit(necessary_df['Country'])\n",
    "\n",
    "# # repeat for other columns\n",
    "# le_industry = LabelEncoder().fit(necessary_df['Industry'])\n",
    "# le_language = LabelEncoder().fit(necessary_df['LanguageHaveWorkedWith'])\n",
    "# le_platform = LabelEncoder().fit(necessary_df['PlatformHaveWorkedWith'])\n",
    "# le_tools = LabelEncoder().fit(necessary_df['ToolsTechHaveWorkedWith'])\n",
    "\n",
    "# data = {\n",
    "#     \"model\": rf_final,\n",
    "#     \"country\": le_country,\n",
    "#     \"industry\": le_industry,\n",
    "#     \"languagehaveworkedwith\": le_language,  # Note the exact key name from your training\n",
    "#     \"platformhaveworkedwith\": le_platform,  # Note the exact key name from your training\n",
    "#     \"toolstechhaveworkedwith\": le_tools    # Note the exact key name from your training\n",
    "# }\n",
    "\n",
    "# with open(\"rf_final_model.pkl\", \"wb\") as file:\n",
    "#     pickle.dump(data, file)\n",
    "\n",
    "# with open(\"rf_final_model.pkl\", \"rb\") as file:\n",
    "#     regressor = pickle.load(file)\n",
    "\n",
    "\n",
    "# print(f\"💾 Model saved to {rf_final}\")\n",
    "\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# import pickle\n",
    "\n",
    "# # List of categorical columns\n",
    "# object_cols = ['Country','Industry','LanguageHaveWorkedWith','PlatformHaveWorkedWith','ToolsTechHaveWorkedWith']\n",
    "\n",
    "# # Dictionary to store separate encoders\n",
    "# encoders = {}\n",
    "\n",
    "# for col in object_cols:\n",
    "#     le = LabelEncoder()\n",
    "#     necessary_df[col] = le.fit_transform(necessary_df[col].astype(str))\n",
    "#     encoders[col] = le   # store encoder for this column\n",
    "\n",
    "# # Save model\n",
    "# with open(\"rf_final_model.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(regressor, f)\n",
    "\n",
    "# # Save encoders\n",
    "# with open(\"encoders.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(encoders, f)\n",
    "\n",
    "# print(\"✅ Model saved to rf_final_model.pkl and encoders saved to encoders.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# import pickle\n",
    "\n",
    "# # Assuming necessary_df is your original DataFrame with raw categorical data\n",
    "# # Fit LabelEncoders on raw categorical columns\n",
    "# le_country = LabelEncoder().fit(necessary_df['Country'])\n",
    "# le_industry = LabelEncoder().fit(necessary_df['Industry'])\n",
    "# le_language = LabelEncoder().fit(necessary_df['LanguageHaveWorkedWith'])\n",
    "# le_platform = LabelEncoder().fit(necessary_df['PlatformHaveWorkedWith'])\n",
    "# le_tools = LabelEncoder().fit(necessary_df['ToolsTechHaveWorkedWith'])\n",
    "\n",
    "# # Create the data dictionary with the model and fitted encoders\n",
    "# data = {\n",
    "#     \"model\": rf_final,\n",
    "#     \"country\": le_country,\n",
    "#     \"industry\": le_industry,\n",
    "#     \"languagehaveworkedwith\": le_language,\n",
    "#     \"platformhaveworkedwith\": le_platform,\n",
    "#     \"toolstechhaveworkedwith\": le_tools\n",
    "# }\n",
    "\n",
    "# # Save the dictionary to a .pkl file\n",
    "# with open(\"rf_final_model.pkl\", \"wb\") as file:\n",
    "#     pickle.dump(data, file)\n",
    "\n",
    "# print(\"✅ Model and encoders saved to rf_final_model.pkl\")\n",
    "\n",
    "# # Optional: Verify the classes\n",
    "# print(\"Country classes:\", le_country.classes_)\n",
    "# print(\"Industry classes:\", le_industry.classes_)\n",
    "# print(\"Language classes:\", le_language.classes_)\n",
    "# print(\"Platform classes:\", le_platform.classes_)\n",
    "# print(\"Tools classes:\", le_tools.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['model', 'country', 'industry', 'languagehaveworkedwith', 'platformhaveworkedwith', 'toolstechhaveworkedwith'])\n",
      "Index(['Compensation', 'ConvertedCompYearly', 'Age', 'EdLevel', 'YearsCodePro',\n",
      "       'Country', 'Industry', 'LanguageHaveWorkedWith',\n",
      "       'PlatformHaveWorkedWith', 'ToolsTechHaveWorkedWith', 'WorkExp'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# import pickle\n",
    "# with open('rf_final_model.pkl', 'rb') as f:\n",
    "#     data = pickle.load(f)\n",
    "# print(data.keys())  # Should show 'model', 'country', 'industry', etc.\n",
    "# print(necessary_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pickle\n",
    "\n",
    "encoders = {}\n",
    "encoders['Country'] = LabelEncoder()\n",
    "encoders['Country'].fit(necessary_df['Country'])\n",
    "# Similarly for other categorical columns\n",
    "\n",
    "# Save encoders\n",
    "with open('encoders.pkl', 'wb') as f:\n",
    "    pickle.dump(encoders, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['Country'])\n"
     ]
    }
   ],
   "source": [
    "with open('encoders.pkl', 'rb') as f:\n",
    "    encoders = pickle.load(f)\n",
    "print(encoders.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Boosting Method**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(\n",
    "    X, y, test_size=0.3, random_state=5\n",
    ")\n",
    "\n",
    "# AdaBoost Regressor with Decision Tree Regressor as base\n",
    "clf_ensemble_Boost = AdaBoostRegressor(\n",
    "    estimator=DT_reg,  # DT_reg should be a DecisionTreeRegressor\n",
    "    n_estimators=18,\n",
    "    random_state=1\n",
    ")\n",
    "clf_ensemble_Boost.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_train_pred = clf_ensemble_Boost.predict(X_train)\n",
    "y_test_pred = clf_ensemble_Boost.predict(X_test)\n",
    "\n",
    "# Regression metrics\n",
    "train_rmse = np.sqrt(metrics.mean_squared_error(y_train, y_train_pred))\n",
    "test_rmse = np.sqrt(metrics.mean_squared_error(y_test, y_test_pred))\n",
    "train_r2 = metrics.r2_score(y_train, y_train_pred)\n",
    "test_r2 = metrics.r2_score(y_test, y_test_pred)\n",
    "\n",
    "print(f\"Train RMSE: {train_rmse:.2f}, R²: {train_r2:.3f}\")\n",
    "print(f\"Test RMSE : {test_rmse:.2f}, R²: {test_r2:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decision Tree using the RandomizedSearchCV\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "\n",
    "# Stage 1: Fast search\n",
    "base_tree = DecisionTreeRegressor(random_state=42)\n",
    "\n",
    "param_dist = {\n",
    "    'n_estimators': [50, 75, 100],                 # fewer for search\n",
    "    'learning_rate': [0.05, 0.1, 0.3, 1.0],\n",
    "    'estimator__max_depth': [3, 5, 7],\n",
    "    'estimator__min_samples_leaf': [2, 4, 6]\n",
    "}\n",
    "\n",
    "rand_search = RandomizedSearchCV(\n",
    "    AdaBoostRegressor(estimator=base_tree, random_state=42),\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=10,          # only 10 random combos\n",
    "    cv=3,               # faster than 5 folds\n",
    "    scoring='r2',\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"🔍 Running fast AdaBoost search...\")\n",
    "rand_search.fit(X_train, y_train)\n",
    "best_params = rand_search.best_params_\n",
    "print(\"✅ Best parameters found:\", best_params)\n",
    "\n",
    "# Stage 2: Retrain with more estimators for accuracy\n",
    "final_tree = DecisionTreeRegressor(\n",
    "    max_depth=best_params['estimator__max_depth'],\n",
    "    min_samples_leaf=best_params['estimator__min_samples_leaf'],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "final_model = AdaBoostRegressor(\n",
    "    estimator=final_tree,\n",
    "    n_estimators=200,                     # more boosting rounds\n",
    "    learning_rate=best_params['learning_rate'],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"🚀 Retraining final AdaBoost model...\")\n",
    "final_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_train_pred = final_model.predict(X_train)\n",
    "y_test_pred = final_model.predict(X_test)\n",
    "\n",
    "# Evaluation\n",
    "train_rmse = np.sqrt(metrics.mean_squared_error(y_train, y_train_pred))\n",
    "test_rmse = np.sqrt(metrics.mean_squared_error(y_test, y_test_pred))\n",
    "train_r2 = metrics.r2_score(y_train, y_train_pred)\n",
    "test_r2 = metrics.r2_score(y_test, y_test_pred)\n",
    "\n",
    "print(f\"\\n📊 Final Model Performance:\")\n",
    "print(f\"Train RMSE: {train_rmse:.2f}, R²: {train_r2:.3f}\")\n",
    "print(f\"Test  RMSE: {test_rmse:.2f}, R²: {test_r2:.3f}\")\n",
    "print(y_test_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample data before preprocessing:\n",
      "                 Age                                            EdLevel  \\\n",
      "72   18-24 years old  Secondary school (e.g. American high school, G...   \n",
      "379  35-44 years old    Master’s degree (M.A., M.S., M.Eng., MBA, etc.)   \n",
      "389  25-34 years old  Some college/university study without earning ...   \n",
      "392  35-44 years old     Professional degree (JD, MD, Ph.D, Ed.D, etc.)   \n",
      "398  45-54 years old  Some college/university study without earning ...   \n",
      "\n",
      "    YearsCodePro                                            Country  \\\n",
      "72             1                                           Pakistan   \n",
      "379            6                                             Turkey   \n",
      "389            7                           United States of America   \n",
      "392           18  United Kingdom of Great Britain and Northern I...   \n",
      "398           30                           United States of America   \n",
      "\n",
      "                                 Industry  \\\n",
      "72                   Software Development   \n",
      "379  Computer Systems Design and Services   \n",
      "389       Transportation, or Supply Chain   \n",
      "392          Retail and Consumer Services   \n",
      "398                            Healthcare   \n",
      "\n",
      "                                LanguageHaveWorkedWith  \\\n",
      "72   Assembly;Bash/Shell (all shells);C;C++;HTML/CS...   \n",
      "379             JavaScript;Julia;Python;SQL;TypeScript   \n",
      "389          HTML/CSS;JavaScript;Python;SQL;TypeScript   \n",
      "392  C#;HTML/CSS;Java;JavaScript;MicroPython;Python...   \n",
      "398  Bash/Shell (all shells);C#;HTML/CSS;JavaScript...   \n",
      "\n",
      "                                PlatformHaveWorkedWith  \\\n",
      "72                              Microsoft Azure;VMware   \n",
      "379  Amazon Web Services (AWS);Cloudflare;Digital O...   \n",
      "389                          Amazon Web Services (AWS)   \n",
      "392                          Amazon Web Services (AWS)   \n",
      "398                                    Microsoft Azure   \n",
      "\n",
      "                           ToolsTechHaveWorkedWith  WorkExp  \\\n",
      "72                             Docker;Homebrew;Pip      3.0   \n",
      "379                              Ansible;Make;Vite      7.0   \n",
      "389                  Homebrew;npm;Pip;Vite;Webpack      8.0   \n",
      "392                      Gradle;Maven (build tool)     18.0   \n",
      "398  Chocolatey;Docker;Visual Studio Solution;Yarn     30.0   \n",
      "\n",
      "     ConvertedCompYearly  \n",
      "72                7322.0  \n",
      "379              91295.0  \n",
      "389             110000.0  \n",
      "392             161044.0  \n",
      "398             195000.0  \n",
      "\n",
      "Data types before encoding:\n",
      "Age                         object\n",
      "EdLevel                     object\n",
      "YearsCodePro                object\n",
      "Country                     object\n",
      "Industry                    object\n",
      "LanguageHaveWorkedWith      object\n",
      "PlatformHaveWorkedWith      object\n",
      "ToolsTechHaveWorkedWith     object\n",
      "WorkExp                    float64\n",
      "ConvertedCompYearly        float64\n",
      "dtype: object\n",
      "\n",
      "Encoder classes after fit (must be strings):\n",
      "Country classes: ['Afghanistan' 'Albania' 'Algeria' 'Andorra' 'Angola' 'Argentina'\n",
      " 'Armenia' 'Australia' 'Austria' 'Azerbaijan' 'Bahrain' 'Bangladesh'\n",
      " 'Barbados' 'Belarus' 'Belgium' 'Benin' 'Bhutan' 'Bolivia'\n",
      " 'Bosnia and Herzegovina' 'Brazil' 'Bulgaria' 'Burkina Faso' 'Cambodia'\n",
      " 'Cameroon' 'Canada' 'Cape Verde' 'Central African Republic' 'Chile'\n",
      " 'China' 'Colombia' 'Congo, Republic of the...' 'Costa Rica' 'Croatia'\n",
      " 'Cuba' 'Cyprus' 'Czech Republic' \"Côte d'Ivoire\"\n",
      " 'Democratic Republic of the Congo' 'Denmark' 'Dominican Republic'\n",
      " 'Ecuador' 'Egypt' 'El Salvador' 'Estonia' 'Ethiopia' 'Finland' 'France'\n",
      " 'Georgia' 'Germany' 'Ghana' 'Greece' 'Guatemala' 'Haiti' 'Honduras'\n",
      " 'Hong Kong (S.A.R.)' 'Hungary' 'Iceland' 'India' 'Indonesia'\n",
      " 'Iran, Islamic Republic of...' 'Iraq' 'Ireland' 'Isle of Man' 'Israel'\n",
      " 'Italy' 'Jamaica' 'Japan' 'Jordan' 'Kazakhstan' 'Kenya' 'Kosovo' 'Kuwait'\n",
      " 'Kyrgyzstan' \"Lao People's Democratic Republic\" 'Latvia' 'Lebanon'\n",
      " 'Libyan Arab Jamahiriya' 'Lithuania' 'Luxembourg' 'Madagascar' 'Malawi'\n",
      " 'Malaysia' 'Maldives' 'Malta' 'Mauritania' 'Mauritius' 'Mexico'\n",
      " 'Mongolia' 'Montenegro' 'Morocco' 'Mozambique' 'Myanmar' 'Namibia'\n",
      " 'Nepal' 'Netherlands' 'New Zealand' 'Nicaragua' 'Nigeria' 'Nomadic'\n",
      " 'Norway' 'Oman' 'Pakistan' 'Palestine' 'Panama' 'Paraguay' 'Peru'\n",
      " 'Philippines' 'Poland' 'Portugal' 'Qatar' 'Republic of Korea'\n",
      " 'Republic of Moldova' 'Republic of North Macedonia' 'Romania'\n",
      " 'Russian Federation' 'Rwanda' 'Saudi Arabia' 'Senegal' 'Serbia'\n",
      " 'Sierra Leone' 'Singapore' 'Slovakia' 'Slovenia' 'South Africa'\n",
      " 'South Korea' 'Spain' 'Sri Lanka' 'Sudan' 'Suriname' 'Sweden'\n",
      " 'Switzerland' 'Syrian Arab Republic' 'Taiwan' 'Thailand'\n",
      " 'Trinidad and Tobago' 'Tunisia' 'Turkey' 'Turkmenistan' 'Uganda'\n",
      " 'Ukraine' 'United Arab Emirates'\n",
      " 'United Kingdom of Great Britain and Northern Ireland'\n",
      " 'United Republic of Tanzania' 'United States of America' 'Uruguay'\n",
      " 'Uzbekistan' 'Venezuela, Bolivarian Republic of...' 'Viet Nam' 'Zambia'\n",
      " 'Zimbabwe']\n",
      "Industry classes: ['Banking/Financial Services' 'Computer Systems Design and Services'\n",
      " 'Energy' 'Fintech' 'Government' 'Healthcare' 'Higher Education'\n",
      " 'Insurance' 'Internet, Telecomm or Information Services' 'Manufacturing'\n",
      " 'Media & Advertising Services' 'Other:' 'Retail and Consumer Services'\n",
      " 'Software Development' 'Transportation, or Supply Chain']\n",
      "Language classes: ['Ada' 'Apex' 'Assembly' 'Bash/Shell (all shells)' 'C' 'C#' 'C++'\n",
      " 'Clojure' 'Cobol' 'Crystal' 'Dart' 'Delphi' 'Elixir' 'Erlang' 'F#'\n",
      " 'Fortran' 'GDScript' 'Go' 'Groovy' 'HTML/CSS' 'Haskell' 'Java'\n",
      " 'JavaScript' 'Julia' 'Kotlin' 'Lisp' 'Lua' 'MATLAB' 'MicroPython' 'OCaml'\n",
      " 'Objective-C' 'PHP' 'Perl' 'PowerShell' 'Python' 'R' 'Ruby' 'Rust' 'SQL'\n",
      " 'Scala' 'Solidity' 'Swift' 'TypeScript']\n",
      "Platform classes: ['Alibaba Cloud' 'Amazon Web Services (AWS)' 'Cloudflare' 'Colocation'\n",
      " 'Databricks' 'Digital Ocean' 'Firebase' 'Fly.io' 'Google Cloud' 'Heroku'\n",
      " 'Hetzner' 'IBM Cloud Or Watson' 'Linode, now Akamai' 'Managed Hosting'\n",
      " 'Microsoft Azure' 'Netlify' 'OVH' 'OpenShift' 'OpenStack'\n",
      " 'Oracle Cloud Infrastructure (OCI)' 'PythonAnywhere' 'Render' 'Scaleway'\n",
      " 'Supabase' 'VMware' 'Vercel' 'Vultr']\n",
      "Tools classes: ['APT' 'Ansible' 'Ant' 'Bun' 'Chef' 'Chocolatey' 'Composer' 'Dagger'\n",
      " 'Docker' 'Godot' 'Google Test' 'Gradle' 'Homebrew' 'Kubernetes' 'MSBuild'\n",
      " 'Make' 'Maven (build tool)' 'Ninja' 'Nix' 'NuGet' 'Pacman' 'Pip' 'Podman'\n",
      " 'Pulumi' 'Puppet' 'Terraform' 'Unity 3D' 'Unreal Engine'\n",
      " 'Visual Studio Solution' 'Vite' 'Webpack' 'Yarn' 'npm' 'pnpm']\n",
      "💾 Model and encoders saved to rf_final_model.pkl\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn import metrics\n",
    "import pickle\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv(\"C:/Users/GOHIL RAJENDRASINH/Downloads/stack-overflow-developer-survey-2024/survey_results_public.csv\")\n",
    "\n",
    "# Select necessary columns\n",
    "necessary_columns = ['Age', 'EdLevel', 'YearsCodePro', 'Country', 'Industry', 'LanguageHaveWorkedWith', 'PlatformHaveWorkedWith', 'ToolsTechHaveWorkedWith', 'WorkExp', 'ConvertedCompYearly']\n",
    "necessary_df = df[necessary_columns].copy()\n",
    "\n",
    "# Handle missing values\n",
    "necessary_df = necessary_df.dropna(subset=necessary_columns)\n",
    "\n",
    "# Verify initial data\n",
    "print(\"Sample data before preprocessing:\")\n",
    "print(necessary_df.head())\n",
    "print(\"\\nData types before encoding:\")\n",
    "print(necessary_df.dtypes)\n",
    "\n",
    "# Education level mapping\n",
    "ed_level_mapping = {\n",
    "    \"Bachelor’s degree (B.A., B.S., B.Eng., etc.)\": 6,\n",
    "    \"Some college/university study without earning a degree\": 4,\n",
    "    \"Master’s degree (M.A., M.S., M.Eng., MBA, etc.)\": 7,\n",
    "    \"Primary/elementary school\": 2,\n",
    "    \"Professional degree (JD, MD, Ph.D, Ed.D, etc.)\": 8,\n",
    "    \"Associate degree (A.A., A.S., etc.)\": 5,\n",
    "    \"Secondary school (e.g. American high school, German Realschule or Gymnasium, etc.)\": 3,\n",
    "    \"Something else\": 1\n",
    "}\n",
    "necessary_df['EdLevel'] = necessary_df['EdLevel'].map(ed_level_mapping).astype(float)\n",
    "\n",
    "# Handle Age column (convert age ranges to midpoints)\n",
    "age_mapping = {\n",
    "    \"Under 18 years old\": 14,\n",
    "    \"18-24 years old\": 21,\n",
    "    \"25-34 years old\": 29.5,\n",
    "    \"35-44 years old\": 39.5,\n",
    "    \"45-54 years old\": 49.5,\n",
    "    \"55-64 years old\": 59.5,\n",
    "    \"65 years or older\": 70\n",
    "}\n",
    "necessary_df['Age'] = necessary_df['Age'].map(age_mapping).astype(float)\n",
    "\n",
    "# Handle multi-value columns by taking the first value\n",
    "for col in ['LanguageHaveWorkedWith', 'PlatformHaveWorkedWith', 'ToolsTechHaveWorkedWith']:\n",
    "    necessary_df[col] = necessary_df[col].apply(lambda x: x.split(';')[0] if isinstance(x, str) else x)\n",
    "\n",
    "# Fit LabelEncoders on RAW string columns (before transformation)\n",
    "le_country = LabelEncoder().fit(necessary_df['Country'])\n",
    "le_industry = LabelEncoder().fit(necessary_df['Industry'])\n",
    "le_language = LabelEncoder().fit(necessary_df['LanguageHaveWorkedWith'])\n",
    "le_platform = LabelEncoder().fit(necessary_df['PlatformHaveWorkedWith'])\n",
    "le_tools = LabelEncoder().fit(necessary_df['ToolsTechHaveWorkedWith'])\n",
    "\n",
    "# Verify classes (must show strings)\n",
    "print(\"\\nEncoder classes after fit (must be strings):\")\n",
    "print(\"Country classes:\", le_country.classes_)\n",
    "print(\"Industry classes:\", le_industry.classes_)\n",
    "print(\"Language classes:\", le_language.classes_)\n",
    "print(\"Platform classes:\", le_platform.classes_)\n",
    "print(\"Tools classes:\", le_tools.classes_)\n",
    "\n",
    "# Transform the DataFrame columns to numbers for training (if re-training)\n",
    "necessary_df['Country'] = le_country.transform(necessary_df['Country'])\n",
    "necessary_df['Industry'] = le_industry.transform(necessary_df['Industry'])\n",
    "necessary_df['LanguageHaveWorkedWith'] = le_language.transform(necessary_df['LanguageHaveWorkedWith'])\n",
    "necessary_df['PlatformHaveWorkedWith'] = le_platform.transform(necessary_df['PlatformHaveWorkedWith'])\n",
    "necessary_df['ToolsTechHaveWorkedWith'] = le_tools.transform(necessary_df['ToolsTechHaveWorkedWith'])\n",
    "\n",
    "# Convert other numeric columns to float\n",
    "for col in ['YearsCodePro', 'WorkExp']:\n",
    "    necessary_df[col] = pd.to_numeric(necessary_df[col], errors='coerce')\n",
    "\n",
    "# Prepare features and target (for reference, but skip training if model is good)\n",
    "X = necessary_df.drop('ConvertedCompYearly', axis=1)\n",
    "y = necessary_df['ConvertedCompYearly']\n",
    "X = X.fillna(X.mean())\n",
    "y = y.fillna(y.mean())\n",
    "\n",
    "# Use existing rf_final (assume it's from your good model)\n",
    "# If re-training is needed, uncomment and run the training block below\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# param_dist = {'n_estimators': [100, 200, 300], 'max_depth': [None, 10, 20, 30], 'min_samples_split': [2, 5, 10], 'min_samples_leaf': [1, 2, 4], 'bootstrap': [True, False]}\n",
    "# rf = RandomForestRegressor(random_state=42)\n",
    "# rf_random = RandomizedSearchCV(estimator=rf, param_distributions=param_dist, n_iter=100, cv=3, verbose=2, random_state=42, n_jobs=-1)\n",
    "# rf_random.fit(X_train, y_train)\n",
    "# rf_final = rf_random.best_estimator_\n",
    "\n",
    "# Save model and encoders to rf_final_model.pkl\n",
    "data = {\n",
    "    \"model\": rf_final,  # Use your existing rf_final\n",
    "    \"country\": le_country,\n",
    "    \"industry\": le_industry,\n",
    "    \"languagehaveworkedwith\": le_language,\n",
    "    \"platformhaveworkedwith\": le_platform,\n",
    "    \"toolstechhaveworkedwith\": le_tools\n",
    "}\n",
    "model_filename = \"rf_final_model.pkl\"\n",
    "with open(model_filename, 'wb') as file:\n",
    "    pickle.dump(data, file)\n",
    "print(f\"💾 Model and encoders saved to {model_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 3506731,
     "sourceId": 6118265,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30587,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
